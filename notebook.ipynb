{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Option Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the realm of options pricing, traditional models often simplify the intricate dynamics of underlying stock movements or are limited to certain constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dynamic Programming\n",
    "\n",
    "In this project we approach theoretical option pricing as a dynamic programming problem in which at each time step we have information $s_t \\in \\mathcal{S}$ such as the underlying price, time till expiry, interest rates, etc. We also have a set of actions we can take at each time step $a \\in \\mathcal{A}$ such as early exercise, hedging adjustment, taking no action, etc. We also have functions defining the cost/reward of taking an action $C(s_t, a_t)$ and probabilty of transitioning from one state to another $P(s_{t + dt} ~|~ s_t, a_t)$ typically independent of $a_t$ and corresponds to price movements of the underlying. Our goal therefore is to find $V^*(s_t)$ which denotes the theoretical value of an option under the assumtion that the holder of the contract takes the most optional actions during the lifetime of the contract. By the Bellman Optimality Principle, $V^*(s_t)$ can be recursively defined as:\n",
    "\n",
    "$$\n",
    "V^*(s_t) = \n",
    "    \\max_{a_t \\in \\mathcal{A}} \n",
    "    \\left\\{\n",
    "        C(s_t, a_t) + \\int_{s_{t+dt} \\in \\mathcal{S}} P(s_{t + dt} ~|~ s_t, a_t) \\cdot V^*(s_{t + dt})  \n",
    "    \\right\\} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Approximate Dynamic Programming\n",
    "To fight the curse of dimensionality inherent with financial problems we will be using approximate dynamic programming which leverages machine learning techniques to approximate $V(s_t)$. The goal in Approximate Dynamic Programming is to parameterize $V(s_t)$ with parameters $\\theta$ such that we minimize the Temporal Difference Value Error which is defined as the absolute difference between the predicted value of a state and the optimal value of the state:\n",
    "\n",
    "$$\n",
    "\\overline{VE} = \n",
    "    V_\\theta(s_t) - \n",
    "    \\max_{a_t \\in \\mathcal{A}} \n",
    "    \\left\\{\n",
    "        C(s_t, a_t) + \\int_{s_{t+dt} \\in \\mathcal{S}} P(s_{t + dt} ~|~ s_t, a_t) \\cdot V_\\theta(s_{t + dt})  \n",
    "    \\right\\} \n",
    "$$\n",
    "\n",
    "We can then use this iteratively perform gradient descent or interpolation to converge towards the true value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Organization\n",
    "The rest of this notebook is organized as follows:\n",
    "\n",
    "- In Section 2 we will use maximum likelihood estimation to model our underlying's price dynamics.\n",
    "- In Section 3 we will describe the various functions as well as the algorithm we will use to approximate $V^*(s_t)$.\n",
    "- In Section 4 we will provide dynamic programming formualtions for the different contracts we will be considering.\n",
    "- In Section 5 we will fit the models and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling The Underlying Asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## European Call Option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Programming Formulation\n",
    "\n",
    "Given a strike price $K$ our option's pay off after exercising at time $t$ is $\\max\\{0, S - K\\}$. Further more since this is an American call option, we can exercise at any time $t \\ge T$. Therefore at each time step we have the choice of exercising and recieving a payoff of $\\max\\{0, S - K\\}$ or not exercising. If we dont exercise, our underlying stock moves to a new price $S_{t + 1}$ with probability $P(S_{t + 1} ~|~ S_t)$ and we have a new problem of whether or not we should exercise. This recursive relationship can be modeled with the following dynamic programm\n",
    "\n",
    "$$\n",
    "V(S, T) = \\max\\left\\{ \\max\\{0, S - K\\}, \\int_{S_\\text{min}}^{S_\\text{max}}V(S_{t + dt}, T - dt) P(S_{t + dt} ~|~ S_t) \\right\\}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
